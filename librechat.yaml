# Configuration version (required)
version: 1.2.5
 
# Cache settings: Set to true to enable caching
cache: true

interface:
  termsOfService:
    externalUrl: "https://example.com/terms"
    openNewTab: true
    modalAcceptance: true
    modalTitle: "Terms of Service"
  mcpServers:
    placeholder: "MCP Servers"
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: true
  prompts: true
  bookmarks: false
  multiConvo: false
  agents: true
  customWelcome: "Hey {{user.name}}! Welcome to LibreChat"
  marketplace:
    use: false
  fileCitations: true


memory:  # this is a bit weird, did not yet fully understand how this works
  disabled: false
  validKeys: ["user_preferences", "conversation_context", "personal_info"]
  tokenLimit: 2000
  personalize: true
  messageWindowSize: 5
  agent:
    provider: "llama.cpp"
    model: "Devstral-Small-2507-Q8_0.gguf"
    instructions: "You are a helpful assistant that remembers user preferences and context."
    model_parameters:
      temperature: 0.7
      max_tokens: 1000

fileConfig:
  endpoints:
#    agents:
#      fileLimit: 5
#      fileSizeLimit: 100  # Maximum size for an individual file in MB
#      totalSizeLimit: 500  # Maximum total size for all files in a single request in MB
    default:
      totalSizeLimit: 100
#      supportedMimeTypes:
#        - "image/.*"
#        - "text/.*"
#        - "application/pdf"
#        - "video/.*"
#        - "application/vnd.ms-excel"
##        - "application/x-yaml"
#        - "audio/mp3"
#        - "audio/mpeg"
#        - "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#        - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
#        - "application/msword"
#    custom:
#      fileLimit: 5
#      fileSizeLimit: 100
#      supportedMimeTypes:
#        - "image/.*"
#        - "application/pdf"
#        - "text/.*"
#        - "text/markdown"
##        - "text/markdown"
  serverFileSizeLimit: 1000
  avatarSizeLimit: 2
  imageGeneration:
    percentage: 100
    px: 1024

endpoints:
  agents:
    # (optional) Default recursion depth for agents, defaults to 25
    recursionLimit: 50
    # (optional) Max recursion depth for agents, defaults to 25
    maxRecursionLimit: 100
    # (optional) Disable the builder interface for agents
    disableBuilder: false
    # (optional) Agent Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
    # capabilities: ["execute_code", "file_search", "actions", "tools", "artifacts", "ocr", "chain", "web_search"]
    capabilities: [ "file_search", "actions", "tools", "chain" ]
  custom:
    - name: "llama.cpp"
      apiKey: "whatever"
      baseURL: "http://host.docker.internal:8080/v1" 
      models:
        default: [
          "Devstral-Small-2507-Q8_0.gguf"
          ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "assistant"
      titleMethod: "completion"
      summaryMethod: "completion"
#shared:
#  titleMethod: "completion"
#  summaryMethod: "completion"
registration:
  socialLogins: []

# Pin a single default model/preset so new chats just work
modelSpecs:
  list:
    - name: "local-llama"
      label: "Local Llama"
#      iconUrl: "/app/client/public/assets/openai.svg"
      default: true
      preset:
        endpoint: "llama.cpp"     # must match the custom endpoint name above
        model: "Devstral-Small-2507-Q8_0.gguf"
#        temperature: 1.0
#        top_p: 0.95
        max_tokens: 128000
    - name: "agents"
      label: "AI Agents"
      description: "Access specialized AI agents for complex tasks."
      showIconInMenu: true
      showIconInHeader: true
      preset:
        model: "Devstral-Small-2507-Q8_0.gguf"
        default: true
        endpoint: "agents"
        max_tokens: 128000
#        temperature: 0.7
#        top_p: 0.95
        modelLabel: "SomethingXO"
        greeting: |
          Select an existing agent or create a new one using the `Agent Builder` in the menu on the right


# Example MCP Servers Object Structure
mcpServers:
  filesystem:
    # type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - /tmp
    iconPath: /home/user/LibreChat/client/public/assets/logo.svg
     The “wrench” icon shows up if no icon is provided as it is the default rendering.
  cyberchef:
    type: streamable-http
    url: http://host.docker.internal:3001/mcp
  weather:
    type: streamable-http
    url: http://host.docker.internal:9999/mcp